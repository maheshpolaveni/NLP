{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "386dfb39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis complete. Results saved to C:\\Users\\Dell\\Downloads\\output.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from textblob import TextBlob\n",
    "import syllapy\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "\n",
    "#download necessary nltk data\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# File paths\n",
    "input_file_path = r'C:\\Users\\Dell\\Downloads\\Input.xlsx'\n",
    "output_file_path = r'C:\\Users\\Dell\\Downloads\\output.xlsx'\n",
    "output_folder_path = r'C:\\Users\\Dell\\Downloads\\articles'\n",
    "\n",
    "# Ensure output folder exists\n",
    "os.makedirs(output_folder_path, exist_ok=True)\n",
    "\n",
    "# Load input URLs\n",
    "def load_input_urls(file_path):\n",
    "    input_data = pd.read_excel(file_path)\n",
    "    return input_data\n",
    "\n",
    "# Extract article text\n",
    "def extract_article_text(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        title = soup.title.string if soup.title else \"No Title\"\n",
    "        paragraphs = soup.find_all('p')\n",
    "        article_text = ' '.join([p.get_text() for p in paragraphs])\n",
    "        return title, article_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching URL {url}: {e}\")\n",
    "        return \"\", \"\"\n",
    "\n",
    "# Save article to file\n",
    "def save_article(url_id, title, text):\n",
    "    file_path = os.path.join(output_folder_path, f\"{url_id}.txt\")\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"{title}\\n\\n{text}\")\n",
    "\n",
    "# Analyze text\n",
    "def analyze_text(article_text):\n",
    "    blob = TextBlob(article_text)\n",
    "    positive_score = sum([sentence.sentiment.polarity for sentence in blob.sentences if sentence.sentiment.polarity > 0])\n",
    "    negative_score = sum([sentence.sentiment.polarity for sentence in blob.sentences if sentence.sentiment.polarity < 0])\n",
    "    polarity_score = blob.sentiment.polarity\n",
    "    subjectivity_score = blob.sentiment.subjectivity\n",
    "\n",
    "    sentences = re.split(r'[.!?]', article_text)\n",
    "    avg_sentence_length = sum(len(sentence.split()) for sentence in sentences) / len(sentences) if sentences else 0\n",
    "\n",
    "    words = article_text.split()\n",
    "    word_count = len(words)\n",
    "    syllable_count = sum(syllapy.count(word) for word in words)\n",
    "    syllable_per_word = syllable_count / word_count if word_count else 0\n",
    "\n",
    "    personal_pronouns = sum(1 for word in words if word.lower() in ['i', 'we', 'you', 'he', 'she', 'they', 'me', 'us'])\n",
    "\n",
    "    complex_word_count = sum(1 for word in words if syllapy.count(word) >= 3)\n",
    "    percentage_complex_words = (complex_word_count / word_count) * 100 if word_count else 0\n",
    "\n",
    "    avg_word_length = sum(len(word) for word in words) / word_count if word_count else 0\n",
    "\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words) if avg_sentence_length and percentage_complex_words else 0\n",
    "\n",
    "    return {\n",
    "        'positive_score': positive_score,\n",
    "        'negative_score': negative_score,\n",
    "        'polarity_score': polarity_score,\n",
    "        'subjectivity_score': subjectivity_score,\n",
    "        'avg_sentence_length': avg_sentence_length,\n",
    "        'percentage_complex_words': percentage_complex_words,\n",
    "        'fog_index': fog_index,\n",
    "        'avg_words_per_sentence': avg_sentence_length,\n",
    "        'complex_word_count': complex_word_count,\n",
    "        'word_count': word_count,\n",
    "        'syllable_per_word': syllable_per_word,\n",
    "        'personal_pronouns': personal_pronouns,\n",
    "        'avg_word_length': avg_word_length\n",
    "    }\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    input_data = load_input_urls(input_file_path)\n",
    "    results = []\n",
    "\n",
    "    for _, row in input_data.iterrows():\n",
    "        url_id = row['URL_ID']\n",
    "        url = row['URL']\n",
    "        title, text = extract_article_text(url)\n",
    "\n",
    "        if text:\n",
    "            save_article(url_id, title, text)\n",
    "            analysis_results = analyze_text(text)\n",
    "            analysis_results['URL_ID'] = url_id\n",
    "            results.append(analysis_results)\n",
    "\n",
    "    # Save results to output file\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_excel(output_file_path, index=False)\n",
    "    print(f\"Analysis complete. Results saved to {output_file_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4320e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
